{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL successfully\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Connect to MySQL with Error Handling\n",
    "try:\n",
    "    connection = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"root\",\n",
    "        database=\"AU_TECH_JOBS\"\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    print(\"Connected to MySQL successfully\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Error: {err}\")\n",
    "    exit(1)  # Exit the script if the connection fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Get all table names in the database\n",
    "cursor.execute(\"SHOW TABLES\")\n",
    "tables = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract table names\n",
    "table_names = [table[0] for table in tables]\n",
    "\n",
    "# Dictionary to hold DataFrames for each table\n",
    "dataframes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Extract required fields from relevant tables\n",
    "required_fields = ['description', 'summary', 'position', 'department', 'job_type']\n",
    "filtered_dataframes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error querying view: 1146 (42S02): Table 'au_tech_jobs.filtered_jobs' doesn't exist\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cursor.execute(\"SELECT * FROM filtered_jobs\")\n",
    "    data = cursor.fetchall()\n",
    "    columns = [col[0] for col in cursor.description]\n",
    "    print(f\"View 'filtered_jobs' has columns: {columns}\")\n",
    "\n",
    "    # Create DataFrame from the view\n",
    "    if data:\n",
    "        filtered_df = pd.DataFrame(data, columns=columns)\n",
    "        print(filtered_df.head())  # Optional: print a few rows to verify\n",
    "    else:\n",
    "        print(\"The view 'filtered_jobs' is empty.\")\n",
    "        filtered_df = pd.DataFrame(columns=columns)\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Error querying view: {err}\")\n",
    "    filtered_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid data to process.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Proceed with Further Analysis\n",
    "if not filtered_df.empty:\n",
    "    # You can now use filtered_df for building your recommendation system\n",
    "    print(\"Data ready for further analysis.\")\n",
    "    rows_before = len(filtered_df)\n",
    "    print(f\"Number of rows before removing duplicates: {rows_before}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No valid data to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after removing duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "#handiling dublicates\n",
    "filtered_df.drop_duplicates(inplace=True)\n",
    "rows_after = len(filtered_df)\n",
    "print(f\"Number of rows after removing duplicates: {rows_after}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TF-IDF and cosine similarity to identify redundant text entries (similar descriptions)\n",
    "if 'job_description' in filtered_df.columns:\n",
    "        tfidf = TfidfVectorizer().fit_transform(filtered_df['job_description'].fillna(''))\n",
    "        similarity_matrix = cosine_similarity(tfidf)\n",
    "\n",
    "        # Identify pairs with high similarity (> 0.9) and drop redundant ones\n",
    "        similar_indices = np.where(similarity_matrix > 0.9)\n",
    "        dropped_indices = set()\n",
    "        for i, j in zip(*similar_indices):\n",
    "            if i != j and i < j and j not in dropped_indices:  # Avoid self-comparison and ensure each pair is only handled once\n",
    "                dropped_indices.add(j)\n",
    "\n",
    "        filtered_df.drop(index=list(dropped_indices), inplace=True)\n",
    "        rows_after = len(filtered_df)\n",
    "        print(f\"Number of rows after removing duplicates: {rows_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Handle Missing Values\n",
    "if 'job_type' in filtered_df.columns and not filtered_df['job_type'].isna().all():\n",
    "    filtered_df['job_type'] = filtered_df['job_type'].fillna(filtered_df['job_type'].mode()[0])\n",
    "if 'department' in filtered_df.columns:\n",
    "    filtered_df['department'] = filtered_df['department'].fillna('Not Specified')\n",
    "if 'job_description' in filtered_df.columns:\n",
    "    filtered_df['job_description'] = filtered_df['job_description'].fillna('Not Specified')\n",
    "if 'summary' in filtered_df.columns:\n",
    "    filtered_df['summary'] = filtered_df['summary'].fillna('Not Specified')\n",
    "if 'company_description' in filtered_df.columns:\n",
    "    filtered_df['company_description'] = filtered_df['company_description'].fillna('Not Specified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Drop rows where critical fields like 'position' are missing\n",
    "if 'position' in filtered_df.columns:\n",
    "    filtered_df.dropna(subset=['position'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Clean HTML Tags from Text Fields\n",
    "def clean_html(text):\n",
    "    try:\n",
    "        return BeautifulSoup(text, \"html.parser\").get_text() if isinstance(text, str) else text\n",
    "    except Exception as e:\n",
    "        print(f\"Warning while cleaning HTML: {e}\")\n",
    "        return text\n",
    "\n",
    "if 'job_description' in filtered_df.columns:\n",
    "    filtered_df['job_description'] = filtered_df['job_description'].apply(clean_html)\n",
    "if 'summary' in filtered_df.columns:\n",
    "    filtered_df['summary'] = filtered_df['summary'].apply(clean_html)\n",
    "if 'company_description' in filtered_df.columns:\n",
    "    filtered_df['company_description'] = filtered_df['company_description'].apply(clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m filtered_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiltered_df_backup.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Step 7: Save the Cleaned Data to an Excel File\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mfiltered_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatabase_export.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopenpyxl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bitri\\Documents\\Au_tech_Jobs\\Job_REC_SYS\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bitri\\Documents\\Au_tech_Jobs\\Job_REC_SYS\\Lib\\site-packages\\pandas\\core\\generic.py:2417\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   2404\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[0;32m   2406\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[0;32m   2407\u001b[0m     df,\n\u001b[0;32m   2408\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2415\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[0;32m   2416\u001b[0m )\n\u001b[1;32m-> 2417\u001b[0m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2419\u001b[0m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2426\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bitri\\Documents\\Au_tech_Jobs\\Job_REC_SYS\\Lib\\site-packages\\pandas\\io\\formats\\excel.py:943\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m    941\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 943\u001b[0m     writer \u001b[38;5;241m=\u001b[39m \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Bitri\\Documents\\Au_tech_Jobs\\Job_REC_SYS\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:57\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     46\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m WriteExcelBuffer \u001b[38;5;241m|\u001b[39m ExcelWriter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[0;32m     59\u001b[0m     engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     62\u001b[0m         path,\n\u001b[0;32m     63\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m     67\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "# Step 7: Save the Cleaned Data to an CSV File\n",
    "filtered_df.to_csv('filtered_df_backup.csv', index=False)\n",
    "# Step 7: Save the Cleaned Data to an Excel File\n",
    "filtered_df.to_excel('database_export.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Jobs:\n",
      "    job_id                                    job_description  \\\n",
      "9      300  Who we are\\n\\nLaunching in 2003, Tyro has grow...   \n",
      "1        2  Who we areLaunching in 2003, Tyro has grown to...   \n",
      "3        4  Who we areLaunching in 2003, Tyro has grown to...   \n",
      "4        5  Who we areLaunching in 2003, Tyro has grown to...   \n",
      "11    1519  Who we areLaunching in 2003, Tyro has grown to...   \n",
      "\n",
      "                                              summary  \\\n",
      "9                                       Not Specified   \n",
      "1   'Who we are Launching in 2003, Tyro has grown ...   \n",
      "3   Who we are Launching in 2003, Tyro has grown t...   \n",
      "4                                       Not Specified   \n",
      "11                                      Not Specified   \n",
      "\n",
      "                            position                 department   job_type  \\\n",
      "9     Desktop Support Engineer (Mac)  Software Engineering - IT  Full-time   \n",
      "1    Security Consultant - Part-Time  Software Engineering - IT  Part-time   \n",
      "3                 Security Architect  Software Engineering - IT  Full-time   \n",
      "4                    Revenue Analyst                    Finance  Full-time   \n",
      "11  Integration Engineer - Ecommerce                    Support  Full-time   \n",
      "\n",
      "   company_name                                company_description  \n",
      "9          tyro  Launching in 2003, Tyro has grown to become Au...  \n",
      "1          tyro  Launching in 2003, Tyro has grown to become Au...  \n",
      "3          tyro  Launching in 2003, Tyro has grown to become Au...  \n",
      "4          tyro  Launching in 2003, Tyro has grown to become Au...  \n",
      "11         tyro  Launching in 2003, Tyro has grown to become Au...  \n"
     ]
    }
   ],
   "source": [
    " # Step 8: Content-Based Filtering - Feature Extraction and Similarity Computation\n",
    "if 'job_description' in filtered_df.columns:\n",
    "    # Compute TF-IDF matrix for job descriptions\n",
    "    tfidf_matrix = TfidfVectorizer(stop_words='english', max_features=5000).fit_transform(filtered_df['job_description'])\n",
    "    # Compute cosine similarity matrix\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "    # Function to get job recommendations based on similarity\n",
    "    def recommend_jobs(job_id, top_n=5):\n",
    "        if job_id not in filtered_df['job_id'].values:\n",
    "            print(f\"Job ID {job_id} not found in the dataset.\")\n",
    "            return pd.DataFrame()\n",
    "        idx = filtered_df.index[filtered_df['job_id'] == job_id].tolist()[0]\n",
    "        similarity_scores = list(enumerate(cosine_sim[idx]))\n",
    "        similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "        similar_jobs_indices = [i[0] for i in similarity_scores[1:top_n+1]]\n",
    "        return filtered_df.iloc[similar_jobs_indices]\n",
    "\n",
    "    # Example usage\n",
    "    job_id_example = filtered_df['job_id'].iloc[0]  # Take the first job ID as an example\n",
    "    recommended_jobs = recommend_jobs(job_id_example)\n",
    "    print(\"Recommended Jobs:\")\n",
    "    print(recommended_jobs)\n",
    "else:\n",
    "    print(\"No valid data to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Based Recommended Jobs:\n",
      "       job_id                                    job_description  \\\n",
      "305      1735  At SafetyCulture, we help businesses get bette...   \n",
      "1900      152  Who we are:Airtasker is Australia’s no. 1 mark...   \n",
      "4172    29763  Software Engineer - Android - Sydney Office - ...   \n",
      "5155    31271  Founded in 2017, Willow is a global technology...   \n",
      "12255   41575  Kasada has an exciting opportunity to join our...   \n",
      "\n",
      "                                                 summary  \\\n",
      "305    'At SafetyCulture, we’re solving problems that...   \n",
      "1900   Our mission at Airtasker is to empower people ...   \n",
      "4172   'Software Engineer - Android - Sydney Office -...   \n",
      "5155   'About us: Founded in 2017, Willow is a global...   \n",
      "12255  'Kasada is Australia’s Cyber Start-up of the y...   \n",
      "\n",
      "                               position                 department   job_type  \\\n",
      "305              Staff Android Engineer                    Support  Full-time   \n",
      "1900          Software Engineer (React)  Software Engineering - IT  Full-time   \n",
      "4172        Software Engineer - Android                                         \n",
      "5155           Senior Android Developer  Software Engineering - IT  Full-time   \n",
      "12255  Senior Android Engineer (Remote)  Software Engineering - IT  Full-time   \n",
      "\n",
      "        company_name                                company_description  \n",
      "305    SafetyCulture  SafetyCulture is an Australian-based, internat...  \n",
      "1900       Airtasker  Airtasker is a trusted community platform that...  \n",
      "4172          Domain  As one of Australia's leading property portals...  \n",
      "5155          Willow  Willow is the digital twin for the built world...  \n",
      "12255         Kasada  Kasada provides the only online traffic integr...  \n",
      "No valid data to process.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Collaborative Filtering - User-Job Interaction Data\n",
    "# Create a user-job interaction matrix\n",
    "try:\n",
    "    cursor.execute(\"SELECT user_id, job_id FROM applied_job\")\n",
    "    interaction_data = cursor.fetchall()\n",
    "    interaction_df = pd.DataFrame(interaction_data, columns=['user_id', 'job_id'])\n",
    "    user_job_matrix = interaction_df.pivot_table(index='user_id', columns='job_id', aggfunc='size', fill_value=0)\n",
    "\n",
    "    # Use SVD for collaborative filtering\n",
    "    svd = TruncatedSVD(n_components=10)\n",
    "    svd_matrix = svd.fit_transform(user_job_matrix)\n",
    "\n",
    "    # Function to get job recommendations for a user based on collaborative filtering\n",
    "    def recommend_jobs_for_user(user_id, top_n=5):\n",
    "        if user_id not in user_job_matrix.index:\n",
    "            print(f\"User ID {user_id} not found in the dataset.\")\n",
    "            return pd.DataFrame()\n",
    "        user_idx = user_job_matrix.index.get_loc(user_id)\n",
    "        similarity_scores = cosine_similarity(svd_matrix[user_idx].reshape(1, -1), svd_matrix)[0]\n",
    "        similar_users_indices = np.argsort(similarity_scores)[::-1]\n",
    "        recommended_jobs = set()\n",
    "        for similar_user_idx in similar_users_indices[1:top_n+1]:\n",
    "            similar_user_id = user_job_matrix.index[similar_user_idx]\n",
    "            user_jobs = interaction_df[interaction_df['user_id'] == similar_user_id]['job_id'].values\n",
    "            recommended_jobs.update(user_jobs)\n",
    "            if len(recommended_jobs) >= top_n:\n",
    "                break\n",
    "        return filtered_df[filtered_df['job_id'].isin(recommended_jobs)].head(top_n)\n",
    "\n",
    "    # Example usage\n",
    "    user_id_example = interaction_df['user_id'].iloc[0]  # Take the first user ID as an example\n",
    "    user_recommended_jobs = recommend_jobs_for_user(user_id_example)\n",
    "    print(\"User-Based Recommended Jobs:\")\n",
    "    print(user_recommended_jobs)\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Error fetching user-job interaction data: {err}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid data to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Recommended Jobs:\n",
      "       job_id                                    job_description  \\\n",
      "305      1735  At SafetyCulture, we help businesses get bette...   \n",
      "1900      152  Who we are:Airtasker is Australia’s no. 1 mark...   \n",
      "4172    29763  Software Engineer - Android - Sydney Office - ...   \n",
      "5155    31271  Founded in 2017, Willow is a global technology...   \n",
      "12255   41575  Kasada has an exciting opportunity to join our...   \n",
      "\n",
      "                                                 summary  \\\n",
      "305    'At SafetyCulture, we’re solving problems that...   \n",
      "1900   Our mission at Airtasker is to empower people ...   \n",
      "4172   'Software Engineer - Android - Sydney Office -...   \n",
      "5155   'About us: Founded in 2017, Willow is a global...   \n",
      "12255  'Kasada is Australia’s Cyber Start-up of the y...   \n",
      "\n",
      "                               position                 department   job_type  \\\n",
      "305              Staff Android Engineer                    Support  Full-time   \n",
      "1900          Software Engineer (React)  Software Engineering - IT  Full-time   \n",
      "4172        Software Engineer - Android                                         \n",
      "5155           Senior Android Developer  Software Engineering - IT  Full-time   \n",
      "12255  Senior Android Engineer (Remote)  Software Engineering - IT  Full-time   \n",
      "\n",
      "        company_name                                company_description  \n",
      "305    SafetyCulture  SafetyCulture is an Australian-based, internat...  \n",
      "1900       Airtasker  Airtasker is a trusted community platform that...  \n",
      "4172          Domain  As one of Australia's leading property portals...  \n",
      "5155          Willow  Willow is the digital twin for the built world...  \n",
      "12255         Kasada  Kasada provides the only online traffic integr...  \n"
     ]
    }
   ],
   "source": [
    "def hybrid_recommend_jobs(user_id, job_id, top_n=5, alpha=0.2):\n",
    "    # Get content-based recommendations\n",
    "    content_recommendations = recommend_jobs(job_id, top_n=top_n)\n",
    "    if content_recommendations.empty:\n",
    "        content_scores = []\n",
    "    else:\n",
    "        content_scores = [(row['job_id'], 1.0) for _, row in content_recommendations.iterrows()]\n",
    "\n",
    "    # Get collaborative filtering recommendations\n",
    "    collaborative_recommendations = recommend_jobs_for_user(user_id, top_n=top_n)\n",
    "    if collaborative_recommendations.empty:\n",
    "        collaborative_scores = []\n",
    "    else:\n",
    "        collaborative_scores = [(row['job_id'], 1.0) for _, row in collaborative_recommendations.iterrows()]\n",
    "\n",
    "    # Combine recommendations with weighted average\n",
    "    combined_scores = {}\n",
    "    for job_id, score in content_scores:\n",
    "        combined_scores[job_id] = combined_scores.get(job_id, 0) + alpha * score\n",
    "    for job_id, score in collaborative_scores:\n",
    "        combined_scores[job_id] = combined_scores.get(job_id, 0) + (1 - alpha) * score\n",
    "\n",
    "    # Sort by combined score and return top_n recommendations\n",
    "    sorted_jobs = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    recommended_job_ids = [job[0] for job in sorted_jobs[:top_n]]\n",
    "    return filtered_df[filtered_df['job_id'].isin(recommended_job_ids)]\n",
    "hybrid_recommendations = hybrid_recommend_jobs(user_id_example, job_id_example)\n",
    "print(\"Hybrid Recommended Jobs:\")\n",
    "print(hybrid_recommendations)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6666666666666666\n",
      "Recall: 0.6666666666666666\n",
      "F1 Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    " # Step 11: Evaluation Metrics for the Recommendation System\n",
    "def evaluate_recommendations(true_interactions, predicted_interactions):\n",
    "    precision = precision_score(true_interactions, predicted_interactions, average='binary')\n",
    "    recall = recall_score(true_interactions, predicted_interactions, average='binary')\n",
    "    f1 = f1_score(true_interactions, predicted_interactions, average='binary')\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Dummy evaluation example (assuming binary relevance)\n",
    "true_interactions = [1, 0, 1, 1, 0]  # True labels indicating relevance of the jobs\n",
    "predicted_interactions = [1, 0, 0, 1, 1]  # Predicted labels from the recommendations\n",
    "precision, recall, f1 = evaluate_recommendations(true_interactions, predicted_interactions)\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job_REC_SYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
